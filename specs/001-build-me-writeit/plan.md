# Implementation Plan: WriteIt - LLM Article Pipeline TUI Application

**Branch**: `001-build-me-writeit` | **Date**: 2025-09-08 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/specs/001-build-me-writeit/spec.md`

## Execution Flow (/plan command scope)
```
1. Load feature spec from Input path
   ‚Üí ‚úÖ Feature spec loaded successfully
2. Fill Technical Context (scan for NEEDS CLARIFICATION)
   ‚Üí ‚úÖ Detected Python TUI application with FastAPI backend
   ‚Üí ‚ùó Need clarification on AI providers, storage preferences, multi-user support
3. Evaluate Constitution Check section below
   ‚Üí ‚úÖ Initial check shows good simplicity, needs library-first verification
   ‚Üí Update Progress Tracking: Initial Constitution Check
4. Execute Phase 0 ‚Üí research.md
   ‚Üí üìã Research TUI frameworks, LLM integration, LMDB patterns
5. Execute Phase 1 ‚Üí contracts, data-model.md, quickstart.md, CLAUDE.md
6. Re-evaluate Constitution Check section
   ‚Üí Update Progress Tracking: Post-Design Constitution Check
7. Plan Phase 2 ‚Üí Describe task generation approach (DO NOT create tasks.md)
8. STOP - Ready for /tasks command
```

**IMPORTANT**: The /plan command STOPS at step 7. Phases 2-4 are executed by other commands:
- Phase 2: /tasks command creates tasks.md
- Phase 3-4: Implementation execution (manual or via tools)

## Summary
WriteIt is a minimalist TUI application that transforms raw content into polished articles through a 4-step LLM pipeline (angles ‚Üí outline ‚Üí draft ‚Üí polish). The system provides human-in-the-loop feedback at each checkpoint, supporting multiple AI models, real-time streaming, and complete artifact history with rewind/branching capabilities.

## Technical Context
**Language/Version**: Python 3.11+  
**Primary Dependencies**: FastAPI, Textual, LMDB, llm.datasette.io, Pydantic, PyYAML  
**Storage**: LMDB for artifacts + YAML files for pipeline configs and exports  
**Testing**: pytest with real LLM integration tests  
**Target Platform**: Linux/macOS terminals, Windows terminal support  
**Project Type**: single - TUI client + embedded FastAPI server  
**Performance Goals**: <200ms step transitions, real-time LLM streaming, <100MB memory usage  
**Constraints**: Must work offline (local models), graceful degradation for network issues  
**Scale/Scope**: Single user, multiple concurrent pipelines, 1000+ saved artifacts per user

## Constitution Check
*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

**Simplicity**:
- Projects: 1 (single TUI app with embedded server)
- Using framework directly? ‚úÖ (Textual for TUI, FastAPI for server)
- Single data model? ‚úÖ (shared models for TUI/server)
- Avoiding patterns? ‚úÖ (no Repository/UoW, direct LMDB access)

**Architecture**:
- EVERY feature as library? üìã (need to verify: storage, pipeline, llm, tui libraries)
- Libraries listed: [to be defined in Phase 1]
- CLI per library: [to be designed with --help/--version/--format]
- Library docs: llms.txt format planned? üìã

**Testing (NON-NEGOTIABLE)**:
- RED-GREEN-Refactor cycle enforced? üìã (will enforce during implementation)
- Git commits show tests before implementation? üìã
- Order: Contract‚ÜíIntegration‚ÜíE2E‚ÜíUnit strictly followed? üìã
- Real dependencies used? ‚úÖ (actual LLM APIs, no mocking)
- Integration tests for: new libraries, contract changes, shared schemas? üìã
- FORBIDDEN: Implementation before test, skipping RED phase ‚úÖ

**Observability**:
- Structured logging included? üìã (plan for pipeline execution logs)
- Frontend logs ‚Üí backend? N/A (single process)
- Error context sufficient? üìã

**Versioning**:
- Version number assigned? üìã (0.1.0 for initial version)
- BUILD increments on every change? üìã
- Breaking changes handled? üìã (pipeline format migration plan)

## Project Structure

### Documentation (this feature)
```
specs/001-build-me-writeit/
‚îú‚îÄ‚îÄ plan.md              # This file (/plan command output)
‚îú‚îÄ‚îÄ research.md          # Phase 0 output (/plan command)
‚îú‚îÄ‚îÄ data-model.md        # Phase 1 output (/plan command)
‚îú‚îÄ‚îÄ quickstart.md        # Phase 1 output (/plan command)
‚îú‚îÄ‚îÄ contracts/           # Phase 1 output (/plan command)
‚îî‚îÄ‚îÄ tasks.md             # Phase 2 output (/tasks command - NOT created by /plan)
```

### Source Code (repository root)
```
# Single project structure
src/
‚îú‚îÄ‚îÄ models/          # Shared data models (Pipeline, Artifact, etc.)
‚îú‚îÄ‚îÄ storage/         # LMDB storage library
‚îú‚îÄ‚îÄ llm/            # LLM integration library
‚îú‚îÄ‚îÄ pipeline/       # Pipeline execution engine
‚îú‚îÄ‚îÄ server/         # FastAPI server
‚îú‚îÄ‚îÄ tui/            # Textual UI components
‚îî‚îÄ‚îÄ cli/            # Main entry points

tests/
‚îú‚îÄ‚îÄ contract/       # API contract tests
‚îú‚îÄ‚îÄ integration/    # End-to-end pipeline tests
‚îî‚îÄ‚îÄ unit/          # Library unit tests

pipelines/          # Example pipeline configurations
styles/            # Writing style primers
```

**Structure Decision**: Option 1 (single project) - TUI app with embedded FastAPI server for internal communication

## Phase 0: Outline & Research
1. **Extract unknowns from Technical Context** above:
   - Research Textual vs other Python TUI frameworks
   - Best practices for FastAPI WebSocket streaming
   - LMDB usage patterns for versioned artifacts
   - llm.datasette.io integration and model management
   - Pipeline state management and branching strategies

2. **Generate and dispatch research agents**:
   ```
   Task: "Research Textual framework for real-time streaming TUI applications"
   Task: "Research LMDB best practices for artifact versioning and branching"
   Task: "Research FastAPI WebSocket streaming patterns for LLM responses"
   Task: "Research llm.datasette.io integration with multiple AI providers"
   Task: "Research pipeline state management for rewind/branch operations"
   ```

3. **Consolidate findings** in `research.md` using format:
   - Decision: [what was chosen]
   - Rationale: [why chosen]
   - Alternatives considered: [what else evaluated]

**Output**: research.md with all NEEDS CLARIFICATION resolved

## Phase 1: Design & Contracts
*Prerequisites: research.md complete*

1. **Extract entities from feature spec** ‚Üí `data-model.md`:
   - Pipeline Configuration (YAML structure)
   - Pipeline Run (execution instance) 
   - Step Artifact (individual LLM responses)
   - Article (final output with history)
   - Style Primer (writing guidelines)
   - User Session (active state)

2. **Generate API contracts** from functional requirements:
   - Pipeline Management: POST /pipeline/start, GET /pipeline/{id}/status
   - Step Execution: POST /step/{id}/execute, WebSocket /step/{id}/stream
   - History/Branching: POST /step/{id}/rewind, POST /step/{id}/fork
   - Persistence: POST /pipeline/{id}/save, GET /artifacts/{id}
   - Output OpenAPI schema to `/contracts/`

3. **Generate contract tests** from contracts:
   - One test file per endpoint group
   - Assert request/response schemas
   - Tests must fail (no implementation yet)

4. **Extract test scenarios** from user stories:
   - Complete 4-step pipeline execution
   - Multi-model response handling
   - Rewind/branch functionality
   - Style consistency validation
   - YAML export with metadata

5. **Update agent file incrementally** (O(1) operation):
   - Run `/scripts/update-agent-context.sh claude`
   - Add WriteIt-specific context and patterns
   - Include pipeline patterns and TUI conventions

**Output**: data-model.md, /contracts/*, failing tests, quickstart.md, CLAUDE.md

## Phase 2: Task Planning Approach
*This section describes what the /tasks command will do - DO NOT execute during /plan*

**Task Generation Strategy**:
- Load `/templates/tasks-template.md` as base
- Generate tasks from Phase 1 design docs (contracts, data model, quickstart)
- Each contract ‚Üí contract test task [P]
- Each entity ‚Üí model creation task [P] 
- Each user story ‚Üí integration test task
- Implementation tasks to make tests pass

**Ordering Strategy**:
- TDD order: Tests before implementation 
- Dependency order: Models ‚Üí Storage ‚Üí LLM ‚Üí Pipeline ‚Üí Server ‚Üí TUI
- Mark [P] for parallel execution (independent files)

**Estimated Output**: 30-35 numbered, ordered tasks in tasks.md

**IMPORTANT**: This phase is executed by the /tasks command, NOT by /plan

## Phase 3+: Future Implementation
*These phases are beyond the scope of the /plan command*

**Phase 3**: Task execution (/tasks command creates tasks.md)  
**Phase 4**: Implementation (execute tasks.md following constitutional principles)  
**Phase 5**: Validation (run tests, execute quickstart.md, performance validation)

## Complexity Tracking
*No constitutional violations identified that require justification*

## Progress Tracking
*This checklist is updated during execution flow*

**Phase Status**:
- [‚úÖ] Phase 0: Research complete (/plan command)
- [‚úÖ] Phase 1: Design complete (/plan command)
- [‚úÖ] Phase 2: Task planning complete (/plan command - describe approach only)
- [ ] Phase 3: Tasks generated (/tasks command)
- [ ] Phase 4: Implementation complete
- [ ] Phase 5: Validation passed

**Gate Status**:
- [‚úÖ] Initial Constitution Check: PASS
- [‚úÖ] Post-Design Constitution Check: PASS
- [‚úÖ] All NEEDS CLARIFICATION resolved
- [ ] Complexity deviations documented

---
*Based on Constitution v2.1.1 - See `/memory/constitution.md`*